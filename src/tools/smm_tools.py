"""
SMM tools for Yuki (SMM manager agent)

Tools:
1. ContentGenerator ‚Äî create, critique, and refine posts
2. YukiMemory ‚Äî 4-layer memory system access
3. LinkedInPublisher ‚Äî publish posts to LinkedIn API v2
"""

import json
import logging
import os
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Type

from crewai.tools import BaseTool
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Helpers: data paths
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _memory_dir() -> str:
    for p in ["/app/data/yuki_memory", "data/yuki_memory"]:
        if os.path.isdir(p):
            return p
    return "data/yuki_memory"


def _load_json(path: str) -> dict:
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def _save_json(path: str, data: dict):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Helpers: LLM calls (free models)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _call_llm(prompt: str, system: str = "", max_tokens: int = 2000) -> Optional[str]:
    """Call LLM via OpenRouter (free) -> Groq (free) -> None."""
    from urllib.request import urlopen, Request
    from urllib.error import HTTPError

    providers = []

    # OpenRouter free models
    or_key = os.getenv("OPENROUTER_API_KEY", "")
    if or_key:
        providers.append({
            "url": "https://openrouter.ai/api/v1/chat/completions",
            "key": or_key,
            "model": "meta-llama/llama-3.3-70b-instruct:free",
        })

    # Groq free
    groq_key = os.getenv("GROQ_API_KEY", "")
    if groq_key:
        providers.append({
            "url": "https://api.groq.com/openai/v1/chat/completions",
            "key": groq_key,
            "model": "llama-3.3-70b-versatile",
        })

    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    for provider in providers:
        try:
            payload = json.dumps({
                "model": provider["model"],
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": 0.7,
            }).encode("utf-8")

            req = Request(
                provider["url"],
                data=payload,
                headers={
                    "Authorization": f"Bearer {provider['key']}",
                    "Content-Type": "application/json",
                },
                method="POST",
            )

            with urlopen(req, timeout=60) as resp:
                data = json.loads(resp.read().decode("utf-8"))
                return data["choices"][0]["message"]["content"]

        except Exception as e:
            logger.warning(f"LLM call failed ({provider['model']}): {e}")
            continue

    return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Helpers: Self-Refine engine (ported from self_refine.py)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

FORBIDDEN_PHRASES = [
    "–ø–æ–ø—Ä–æ–±—É–π—Ç–µ", "–≤–æ–∑–º–æ–∂–Ω–æ", "–≤–µ—Ä—å—Ç–µ –≤ —Å–µ–±—è",
    "–∫–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫ —É–Ω–∏–∫–∞–ª–µ–Ω", "—Å–µ–∫—Ä–µ—Ç —É—Å–ø–µ—Ö–∞",
    "–±—ã—Å—Ç—Ä–æ –∏ –ª–µ–≥–∫–æ", "–º–Ω–æ–≥–∏–µ –ª—é–¥–∏",
    "—Å–µ–≥–æ–¥–Ω—è –ø–æ–≥–æ–≤–æ—Ä–∏–º", "—Ö–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å—è",
    "—è –≤–∏–¥–µ–ª–∞ —ç—Ç–æ 10 000 —Ä–∞–∑", "—è –≤–∏–¥–µ–ª–∞ —ç—Ç–æ —Ç—ã—Å—è—á–∏ —Ä–∞–∑",
    "–∑–≤—É—á–∏—Ç –∂—ë—Å—Ç–∫–æ? –æ–∫. –Ω–æ —ç—Ç–æ –ø—Ä–∞–≤–¥–∞",
    "—Ö–≤–∞—Ç–∏—Ç –∂–µ–≤–∞—Ç—å —Å–æ–ø–ª–∏",
    "–≤ —ç—Ç–æ–º –ø–æ—Å—Ç–µ", "–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä—ë–º—Å—è",
    "–Ω–µ —Å–µ–∫—Ä–µ—Ç, —á—Ç–æ", "–∫–∞–∫ –º—ã –≤—Å–µ –∑–Ω–∞–µ–º",
    "–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ", "–≤ –Ω–∞—à–µ –≤—Ä–µ–º—è",
    "–Ω–∏ –¥–ª—è –∫–æ–≥–æ –Ω–µ —Å–µ–∫—Ä–µ—Ç", "–≤—Å–µ–º –∏–∑–≤–µ—Å—Ç–Ω–æ",
    "–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ —Ö–æ—á—É —Å–∫–∞–∑–∞—Ç—å", "–ø–æ–¥–≤–æ–¥—è –∏—Ç–æ–≥",
    "–¥—Ä—É–∑—å—è", "–¥–æ—Ä–æ–≥–∏–µ –¥—Ä—É–∑—å—è",
    "–≤ —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ", "—Å–µ–≥–æ–¥–Ω—è —è —Ä–∞—Å—Å–∫–∞–∂—É",
    "–≥–ª–∞–≤–Ω–∞—è –º—ã—Å–ª—å –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º",
    "–ø—Ä–∏—á–∏–Ω–∞ —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è",
    "–æ—Ü–µ–Ω–∏—Ç–µ —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏", "–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —Ä–æ—Å—Ç–∞",
    "–ø–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –≤–∞–∂–Ω–æ", "–Ω–µ –∑–∞–±—ã–≤–∞–π—Ç–µ –æ —Ç–æ–º",
    "–ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å —Å–µ–±–µ –≤–æ–ø—Ä–æ—Å",
    "—á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏",
    "–≤–∞–º –Ω—É–∂–Ω–æ –Ω–∞—É—á–∏—Ç—å—Å—è", "–≤—ã –¥–æ–ª–∂–Ω—ã –ø–æ–Ω–∏–º–∞—Ç—å",
]


def _strip_non_cyrillic(text: str) -> str:
    """Remove CJK characters and other non-expected unicode from generated text.

    Keeps: Cyrillic, Latin, digits, common punctuation, emoji, arrows.
    Removes: CJK (Chinese/Japanese/Korean), Arabic, Thai, etc.
    """
    import re
    # Remove CJK Unified Ideographs, CJK Compatibility, Hangul, Katakana, Hiragana
    text = re.sub(r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af\u3000-\u303f]', '', text)
    # Clean up any resulting double-spaces or orphaned arrows
    text = re.sub(r'‚Üí\s*‚Üí', '‚Üí', text)
    text = re.sub(r'  +', ' ', text)
    # Clean up lines that became just whitespace
    lines = text.split('\n')
    cleaned = []
    for line in lines:
        stripped = line.strip()
        if stripped or not cleaned or cleaned[-1] != '':
            cleaned.append(line)
    return '\n'.join(cleaned)


def _evaluate_hook(content: str) -> Tuple[float, List[str]]:
    issues = []
    score = 0.5
    lines = content.strip().split("\n")
    first_lines = " ".join(lines[:3]).lower()
    if re.search(r"\d+%|\d+\s*(–ª–µ—Ç|—Ä–∞–∑|—á–µ–ª–æ–≤–µ–∫|–∫–æ–º–ø–∞–Ω–∏|—Ä–µ–∑—é–º–µ)", first_lines):
        score += 0.3
    if "?" in first_lines:
        score += 0.1
    emotion_words = ["–Ω–∏–∫–æ–≥–¥–∞", "–≤—Å–µ–≥–¥–∞", "–∫–∞–∂–¥—ã–π", "–æ—à–∏–±–∫–∞", "–ø—Ä–æ–±–ª–µ–º–∞", "–ø—Ä–∞–≤–¥–∞"]
    if any(w in first_lines for w in emotion_words):
        score += 0.1
    boring_starts = ["—Å–µ–≥–æ–¥–Ω—è —è", "—Ö–æ—á—É —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å", "–≤ —ç—Ç–æ–º –ø–æ—Å—Ç–µ", "–ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º",
                     "–º–Ω–æ–≥–∏–µ –ª—é–¥–∏", "–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ", "–Ω–∏ –¥–ª—è –∫–æ–≥–æ –Ω–µ —Å–µ–∫—Ä–µ—Ç",
                     "–Ω–µ —Å–µ–∫—Ä–µ—Ç, —á—Ç–æ", "–∫–∞–∫ –º—ã –≤—Å–µ –∑–Ω–∞–µ–º"]
    if any(s in first_lines for s in boring_starts):
        score -= 0.3
        issues.append("HOOK: –°–∫—É—á–Ω–æ–µ –Ω–∞—á–∞–ª–æ")
    return min(1.0, max(0.0, score)), issues


def _evaluate_specificity(content: str) -> Tuple[float, List[str]]:
    issues = []
    score = 0.3
    numbers = re.findall(r"\d+", content)
    if len(numbers) >= 3:
        score += 0.3
    elif len(numbers) >= 1:
        score += 0.15
    else:
        issues.append("SPECIFICITY: –ù–µ—Ç —Ü–∏—Ñ—Ä –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    examples = ["–Ω–∞–ø—Ä–∏–º–µ—Ä", "–ø—Ä–∏–º–µ—Ä", "—Å–ª—É—á–∞–π", "–∫–ª–∏–µ–Ω—Ç", "–∫–∞–Ω–¥–∏–¥–∞—Ç", "—Å–∏—Ç—É–∞—Ü–∏—è"]
    if any(w in content.lower() for w in examples):
        score += 0.2
    if re.search(r"\d+%", content):
        score += 0.2
    return min(1.0, max(0.0, score)), issues


def _evaluate_structure(content: str) -> Tuple[float, List[str]]:
    issues = []
    score = 0.3
    paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
    if len(paragraphs) >= 4:
        score += 0.3
    elif len(paragraphs) >= 2:
        score += 0.15
    else:
        issues.append("STRUCTURE: –ú–∞–ª–æ –∞–±–∑–∞—Ü–µ–≤, –Ω–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã")
    if re.search(r"[‚Üí‚Ä¢\-\*]", content):
        score += 0.2
    if re.search(r"‚Äî\s*\n.*–°–ë–û–†–ö–ê", content):
        score += 0.2
    else:
        issues.append("STRUCTURE: –ù–µ—Ç –ø–æ–¥–ø–∏—Å–∏ ¬´‚Äî –ê–≤—Ç–æ—Ä, –°–ë–û–†–ö–ê¬ª")
    return min(1.0, max(0.0, score)), issues


def _evaluate_tone(content: str, author: str = "") -> Tuple[float, List[str]]:
    issues = []
    score = 0.6
    soft = ["–º–æ–∂–µ—Ç –±—ã—Ç—å", "–Ω–∞–≤–µ—Ä–Ω–æ–µ", "–∫–∞–∂–µ—Ç—Å—è", "–≤—Ä–æ–¥–µ –±—ã", "–Ω–µ —É–≤–µ—Ä–µ–Ω"]
    if any(w in content.lower() for w in soft):
        score -= 0.3
        issues.append("TONE: –ù–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏")
    direct = ["–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç", "—Ñ–∞–∫—Ç", "—Ü–∏—Ñ—Ä—ã", "—Ñ–æ—Ä–º—É–ª–∞", "–≤–æ—Ç –ø—Ä–∏–º–µ—Ä"]
    if any(w in content.lower() for w in direct):
        score += 0.2
    # Penalize messianic/preachy tone
    preachy = ["–æ—Ü–µ–Ω–∏—Ç–µ —Å–≤–æ–∏", "–ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å", "–ø–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –≤–∞–∂–Ω–æ",
               "–≤–∞–º –Ω—É–∂–Ω–æ", "–≤—ã –¥–æ–ª–∂–Ω—ã", "–≤–∞–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å", "–Ω–µ –∑–∞–±—ã–≤–∞–π—Ç–µ",
               "–ø–æ–¥—É–º–∞–π—Ç–µ –æ —Ç–æ–º", "–∑–∞–¥–∞–π—Ç–µ —Å–µ–±–µ –≤–æ–ø—Ä–æ—Å"]
    preachy_found = [p for p in preachy if p in content.lower()]
    if preachy_found:
        score -= 0.3
        issues.append(f"TONE: –ú–µ–Ω—Ç–æ—Ä—Å–∫–∏–π/–ø–æ—É—á–∞—é—â–∏–π —Ç–æ–Ω: {', '.join(preachy_found[:3])}")
    length = len(content)
    if 1200 <= length <= 3000:
        score += 0.2
    elif length < 800:
        issues.append(f"TONE: –ü–æ—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({length} —Å–∏–º–≤–æ–ª–æ–≤, –Ω—É–∂–Ω–æ 1200+)")
    # Check for CTA at the end
    last_lines = content.strip().split("\n")[-3:]
    last_text = " ".join(last_lines).lower()
    has_cta = "?" in last_text or any(w in last_text for w in [
        "—Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ", "–Ω–∞–ø–∏—à–∏—Ç–µ", "—Å–∫–∏–Ω—å—Ç–µ", "–¥–µ–ª–∏—Ç–µ—Å—å", "–∞ –≤—ã", "–∞ —É –≤–∞—Å",
        "–ø–∏—à–∏—Ç–µ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä", "–∫–∞–∫–æ–π –≤–∞—à", "—á—Ç–æ –¥—É–º–∞–µ—Ç–µ",
    ])
    if not has_cta:
        score -= 0.2
        issues.append("TONE: –ù–µ—Ç CTA/–≤–æ–ø—Ä–æ—Å–∞ –≤ –∫–æ–Ω—Ü–µ –ø–æ—Å—Ç–∞")
    return min(1.0, max(0.0, score)), issues


def _evaluate_forbidden(content: str) -> Tuple[float, List[str]]:
    issues = []
    found = [p for p in FORBIDDEN_PHRASES if p.lower() in content.lower()]
    if found:
        issues.append(f"FORBIDDEN: –ù–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã: {', '.join(found)}")
        return 0.0, issues
    return 1.0, []


def _critique_content(content: str, topic: str = "", author: str = "") -> Dict:
    """Full rule-based critique."""
    hook_score, hook_issues = _evaluate_hook(content)
    spec_score, spec_issues = _evaluate_specificity(content)
    struct_score, struct_issues = _evaluate_structure(content)
    tone_score, tone_issues = _evaluate_tone(content, author)
    forb_score, forb_issues = _evaluate_forbidden(content)

    overall = (
        hook_score * 0.25
        + spec_score * 0.20
        + struct_score * 0.20
        + tone_score * 0.20
        + forb_score * 0.15
    )

    all_issues = hook_issues + spec_issues + struct_issues + tone_issues + forb_issues
    passed = overall >= 0.8 and forb_score == 1.0

    return {
        "overall_score": round(overall, 3),
        "scores": {
            "hook": round(hook_score, 3),
            "specificity": round(spec_score, 3),
            "structure": round(struct_score, 3),
            "tone": round(tone_score, 3),
            "forbidden": round(forb_score, 3),
        },
        "issues": all_issues,
        "passed": passed,
        "length": len(content),
    }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Tool 1: Content Generator
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class ContentGeneratorInput(BaseModel):
    action: str = Field(
        ...,
        description=(
            "Action: 'generate' (create a post ‚Äî needs topic, author), "
            "'critique' (evaluate existing content ‚Äî needs content), "
            "'refine' (improve content ‚Äî needs content, optional topic/author)"
        ),
    )
    topic: Optional[str] = Field(None, description="Post topic (e.g., '—Ä–µ–∑—é–º–µ', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ', 'LinkedIn –ø—Ä–æ—Ñ–∏–ª—å')")
    author: Optional[str] = Field(
        None,
        description="Author: 'kristina' (–ö—Ä–∏—Å—Ç–∏–Ω–∞ –ñ—É–∫–æ–≤–∞) or 'tim' (–¢–∏–º –ó–∏–Ω–∏–Ω). Default: kristina"
    )
    content: Optional[str] = Field(None, description="Existing content to critique or refine")
    platform: Optional[str] = Field(None, description="Platform: 'linkedin' (default), 'telegram'")


class ContentGenerator(BaseTool):
    name: str = "Content Generator"
    description: str = (
        "Creates, critiques, and refines SMM posts for –°–ë–û–†–ö–ê. "
        "Uses 6-part structure (hook, problem, story, insight, action, conclusion). "
        "Actions: generate, critique, refine."
    )
    args_schema: Type[BaseModel] = ContentGeneratorInput

    def _run(self, action: str, topic: str = None, author: str = None,
             content: str = None, platform: str = None) -> str:

        author_name = "–ö—Ä–∏—Å—Ç–∏–Ω–∞ –ñ—É–∫–æ–≤–∞" if (author or "kristina") == "kristina" else "–¢–∏–º –ó–∏–Ω–∏–Ω"
        platform = platform or "linkedin"

        if action == "critique":
            if not content:
                return "Error: need content to critique"
            result = _critique_content(content, topic or "", author_name)
            lines = [f"CRITIQUE RESULT (score: {result['overall_score']:.2f}, passed: {result['passed']})"]
            for k, v in result["scores"].items():
                lines.append(f"  {k}: {v:.2f}")
            if result["issues"]:
                lines.append("Issues:")
                for issue in result["issues"]:
                    lines.append(f"  - {issue}")
            lines.append(f"Length: {result['length']} chars")
            return "\n".join(lines)

        if action == "refine":
            if not content:
                return "Error: need content to refine"
            return self._refine(content, topic or "", author_name)

        if action == "generate":
            if not topic:
                return "Error: need topic to generate"
            return self._generate(topic, author_name, platform)

        return f"Unknown action: {action}"

    def _generate(self, topic: str, author: str, platform: str) -> str:
        """Generate a post using LLM with self-refine."""
        # Load memory data for context
        mem_dir = _memory_dir()
        brand = _load_json(os.path.join(mem_dir, "semantic", "brand_voice.json"))
        vocab = _load_json(os.path.join(mem_dir, "semantic", "vocabulary.json"))
        rules = _load_json(os.path.join(mem_dir, "procedural", "rules.json"))

        author_key = "kristina" if "–ö—Ä–∏—Å—Ç–∏–Ω–∞" in author else "tim"
        author_info = brand.get("authors", {}).get(author_key, {})

        forbidden = vocab.get("forbidden_phrases", [])
        rules_text = "\n".join(f"- {r['rule']}" for r in rules.get("rules", []))

        system_prompt = f"""–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞ –°–ë–û–†–ö–ê (–∫–ª—É–± –∫–∞—Ä—å–µ—Ä–Ω–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã).

–ü–∏—à–µ—à—å –æ—Ç –∏–º–µ–Ω–∏: {author} ({author_info.get('role', '')})
–ì–æ–ª–æ—Å: {author_info.get('voice', '–ü—Ä—è–º–æ–π, —É–≤–µ—Ä–µ–Ω–Ω—ã–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π')}

‚ö†Ô∏è –Ø–ó–´–ö: –°–¢–†–û–ì–û –†–£–°–°–ö–ò–ô. –ù–∏ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º, —è–ø–æ–Ω—Å–∫–æ–º, –∫–æ—Ä–µ–π—Å–∫–æ–º –∏–ª–∏ –ª—é–±–æ–º –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ –∫—Ä–æ–º–µ —Ä—É—Å—Å–∫–æ–≥–æ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ (—Ç–æ–ª—å–∫–æ —Ç–µ—Ä–º–∏–Ω—ã –≤—Ä–æ–¥–µ LinkedIn, AI, HR). –í—Å–µ —Å—Ç—Ä–µ–ª–∫–∏ –ø–∏—à–∏ –∫–∞–∫ ‚Üí, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã.

‚õî –ê–ë–°–û–õ–Æ–¢–ù–´–ô –ó–ê–ü–†–ï–¢ –ù–ê –ú–ï–°–°–ò–ê–ù–°–¢–í–û:
- –ù–ï –ü–û–£–ß–ê–ô —á–∏—Ç–∞—Ç–µ–ª—è. –¢—ã –ù–ï —É—á–∏—Ç–µ–ª—å, –ù–ï –≥—É—Ä—É, –ù–ï –º–µ–Ω—Ç–æ—Ä.
- –ù–ï –î–ê–í–ê–ô –°–û–í–ï–¢–û–í –≤ —Å—Ç–∏–ª–µ ¬´–æ—Ü–µ–Ω–∏—Ç–µ —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏¬ª, ¬´–ø–æ–¥—É–º–∞–π—Ç–µ –æ...¬ª, ¬´–ø–æ–ø—Ä–æ–±—É–π—Ç–µ...¬ª
- –¢—ã –¥–µ–ª–∏—à—å—Å—è –°–í–û–ò–ú –æ–ø—ã—Ç–æ–º –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º–∏, –∞ –Ω–µ —É—á–∏—à—å –∂–∏–∑–Ω–∏.
- –¢–æ–Ω = –∫–æ–ª–ª–µ–≥–∞ –∑–∞ –∫–æ—Ñ–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é, –∞ –ù–ï –∫–æ—É—á –Ω–∞ —Å—Ü–µ–Ω–µ.
- –í–º–µ—Å—Ç–æ ¬´–í–∞–º –Ω—É–∂–Ω–æ...¬ª –ø–∏—à–∏ ¬´–Ø –∑–∞–º–µ—Ç–∏–ª, —á—Ç–æ...¬ª –∏–ª–∏ ¬´–£ –Ω–∞—Å –≤ –∫–æ–º–∞–Ω–¥–µ...¬ª
- –í–º–µ—Å—Ç–æ ¬´–û—Ü–µ–Ω–∏—Ç–µ —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏¬ª –ø–∏—à–∏ ¬´–ö–æ–≥–¥–∞ —è –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ —Å–º–æ—Ç—Ä–µ–ª —Å–≤–æ–π —Å–ø–∏—Å–æ–∫ —Å–∫–∏–ª–ª–æ–≤, –≤—ã—è—Å–Ω–∏–ª–æ—Å—å...¬ª
- –ù–∏–∫–∞–∫–∏—Ö ¬´5 —à–∞–≥–æ–≤ –∫ —É—Å–ø–µ—Ö—É¬ª, ¬´3 –ø—Ä–∞–≤–∏–ª–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏¬ª, ¬´–∫–∞–∫ —Å—Ç–∞—Ç—å –ª—É—á—à–µ¬ª.

üé£ –ö–†–Æ–ß–û–ö (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û, –ü–ï–†–í–ê–Ø –°–¢–†–û–ö–ê):
–ü–æ—Å—Ç –ù–ê–ß–ò–ù–ê–ï–¢–°–Ø —Å –∫–æ—Ä–æ—Ç–∫–æ–≥–æ, —è—Ä–∫–æ–≥–æ, –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –≤–±—Ä–æ—Å–∞. 1 —Å—Ç—Ä–æ–∫–∞ max.
–ü—Ä–∏–º–µ—Ä—ã —Ö–æ—Ä–æ—à–∏—Ö –∫—Ä—é—á–∫–æ–≤:
- ¬´73% —Ä–µ–∑—é–º–µ –ª–µ—Ç—è—Ç –≤ –∫–æ—Ä–∑–∏–Ω—É –∑–∞ 6 —Å–µ–∫—É–Ω–¥.¬ª
- ¬´–í—á–µ—Ä–∞ —É–≤–æ–ª–∏–ª–∏ –ª—É—á—à–µ–≥–æ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –≤ –∫–æ–º–∞–Ω–¥–µ. –ó–∞ —á—Ç–æ? –ó–∞ –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º.¬ª
- ¬´LinkedIn –ø—Ä–µ–≤—Ä–∞—Ç–∏–ª—Å—è –≤ —è—Ä–º–∞—Ä–∫—É —Ç—â–µ—Å–ª–∞–≤–∏—è. –ò —è —á–∞—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã.¬ª
- ¬´–ú–æ–π –∫–ª–∏–µ–Ω—Ç –ø–æ–ª—É—á–∏–ª –æ—Ñ—Ñ–µ—Ä –Ω–∞ 40% –≤—ã—à–µ. –°–µ–∫—Ä–µ—Ç? –û–Ω –≤—Ä–∞–ª. –ü–æ—á—Ç–∏.¬ª
–ù–ï –ù–ê–ß–ò–ù–ê–ô —Å: ¬´–ú–Ω–æ–≥–∏–µ –ª—é–¥–∏¬ª, ¬´–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ¬ª, ¬´–°–µ–≥–æ–¥–Ω—è –ø–æ–≥–æ–≤–æ—Ä–∏–º¬ª, ¬´–•–æ—á—É –ø–æ–¥–µ–ª–∏—Ç—å—Å—è¬ª.

üìê –°–¢–†–£–ö–¢–£–†–ê (–≤—Å–µ 6 —á–∞—Å—Ç–µ–π):
1. –ö—Ä—é—á–æ–∫ (1 —Å—Ç—Ä–æ–∫–∞) ‚Äî –ø—Ä–æ–≤–æ–∫–∞—Ü–∏—è, —Ñ–∞–∫—Ç —Å —Ü–∏—Ñ—Ä–æ–π, –ø–∞—Ä–∞–¥–æ–∫—Å. –ö–æ—Ä–æ—Ç–∫–∞—è. –Ø—Ä–∫–∞—è. –ë—å—ë—Ç.
2. –ü—Ä–æ–±–ª–µ–º–∞ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –±–æ–ª—å —Å –¥–µ—Ç–∞–ª—è–º–∏ –∏ –º–∞—Å—à—Ç–∞–±–æ–º.
3. –ò—Å—Ç–æ—Ä–∏—è/–ö–µ–π—Å (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) ‚Äî –ö–û–ù–ö–†–ï–¢–ù–´–ô –ø—Ä–∏–º–µ—Ä —Å –∏–º–µ–Ω–µ–º, —Ü–∏—Ñ—Ä–∞–º–∏, –¥–µ—Ç–∞–ª—è–º–∏. ¬´–ú–æ–π –∫–æ–ª–ª–µ–≥–∞ –î–∏–º–∞¬ª > ¬´–º–Ω–æ–≥–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã¬ª.
4. –ò–Ω—Å–∞–π—Ç (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) ‚Äî –ø–æ—á–µ–º—É —Ç–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –º–µ—Ö–∞–Ω–∏–∫–∞. –î–µ–ª–∏—Å—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º, –∞ –ù–ï –Ω—Ä–∞–≤–æ—É—á–µ–Ω–∏–µ–º.
5. –ü—Ä–∞–∫—Ç–∏–∫–∞ (3-5 –ø—É–Ω–∫—Ç–æ–≤ ‚Üí) ‚Äî —á—Ç–æ –ö–û–ù–ö–†–ï–¢–ù–û –¥–µ–ª–∞—Ç—å. –ö–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç ‚Äî –¥–µ–π—Å—Ç–≤–∏–µ, –Ω–µ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è.
6. CTA / –§–∏–Ω–∞–ª (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∑–∞–∫–∞–Ω—á–∏–≤–∞–π –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–æ–ø—Ä–æ—Å–æ–º –ò–õ–ò –ø—Ä–∏–∑—ã–≤–æ–º –∫ –¥–µ–π—Å—Ç–≤–∏—é.

üîö CTA –í –ö–û–ù–¶–ï (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û):
–ü–æ—Å–ª–µ–¥–Ω–∏–µ 1-2 —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å—Ç–∞ ‚Äî –í–°–ï–ì–î–ê –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø—Ä–∏–∑—ã–≤. –ü—Ä–∏–º–µ—Ä—ã:
- ¬´–ê —É –≤–∞—Å –≤ –∫–æ–º–∞–Ω–¥–µ –µ—Å—Ç—å —Ç–∞–∫–∏–µ "–∑–≤—ë–∑–¥—ã"? –ß—Ç–æ —Å –Ω–∏–º–∏ –¥–µ–ª–∞–µ—Ç–µ?¬ª
- ¬´–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ö ‚Äî —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—ã –ø–µ—Ä–µ–¥–µ–ª—ã–≤–∞–ª–∏ —Ä–µ–∑—é–º–µ –≤ —ç—Ç–æ–º –≥–æ–¥—É?¬ª
- ¬´–°–∫–∏–Ω—å—Ç–µ —Å–≤–æ–π LinkedIn ‚Äî —Ä–∞–∑–±–µ—Ä—É –±–µ—Å–ø–ª–∞—Ç–Ω–æ –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–æ—Ñ–∏–ª–µ–π.¬ª
- ¬´–ù–∞–ø–∏—à–∏—Ç–µ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º ‚Äî —á—Ç–æ –¥–ª—è –≤–∞—Å –≥–ª–∞–≤–Ω–æ–µ –≤ —Ä–∞–±–æ—Ç–µ?¬ª
–ù–ï –ó–ê–ö–ê–ù–ß–ò–í–ê–ô –ø—Ä–æ—Å—Ç–æ –≤—ã–≤–æ–¥–æ–º –±–µ–∑ –≤–æ–ø—Ä–æ—Å–∞/–¥–µ–π—Å—Ç–≤–∏—è.

–ü–†–ê–í–ò–õ–ê:
{rules_text}

–ó–ê–ü–†–ï–©–ï–ù–û: {', '.join(forbidden[:10])}

–ê–ù–¢–ò-–®–ê–ë–õ–û–ù ‚Äî –ù–ï –ü–ò–®–ò –¢–ê–ö:
- ¬´–ú–Ω–æ–≥–∏–µ –ª—é–¥–∏ –¥–æ —Å–∏—Ö –ø–æ—Ä —É–±–µ–∂–¥–µ–Ω—ã...¬ª ‚Äî –°–ö–£–ß–ù–û.
- ¬´–ì–ª–∞–≤–Ω–∞—è –º—ã—Å–ª—å –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ...¬ª ‚Äî –ö–ê–ù–¶–ï–õ–Ø–†–ò–¢.
- ¬´–û—Ü–µ–Ω–∏—Ç–µ —Å–≤–æ–∏ –Ω–∞–≤—ã–∫–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —Ä–æ—Å—Ç–∞¬ª ‚Äî –ú–ï–°–°–ò–ê–ù–°–¢–í–û. –¢—ã –Ω–µ –∫–æ—É—á.
- ¬´–ß—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏, –Ω–∞–º –Ω—É–∂–Ω–æ...¬ª ‚Äî –û–ë–©–û.
- ¬´–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ —Ö–æ—á—É —Å–∫–∞–∑–∞—Ç—å...¬ª ‚Äî –õ–ò–®–ù–ï–ï.
- ¬´–ü–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –≤–∞–∂–Ω–æ...¬ª ‚Äî –ü–û–£–ß–ï–ù–ò–ï. –ù–µ –Ω–∞–¥–æ.
- ¬´–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å —Å–µ–±–µ –≤–æ–ø—Ä–æ—Å...¬ª ‚Äî –¢–´ –ù–ï –¢–†–ï–ù–ï–†.
- –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –æ–¥–Ω—É –º—ã—Å–ª—å —Ä–∞–∑–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö –∞–±–∑–∞—Ü–∞—Ö.

–°–¢–ò–õ–¨:
- –ü–∏—à–∏ –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫ –∑–∞ –∫–æ—Ñ–µ, –ù–ï –∫–∞–∫ ChatGPT –∏ –ù–ï –∫–∞–∫ –∫–æ—É—á –Ω–∞ —Å—Ü–µ–Ω–µ
- –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –æ–±–æ—Ä–æ—Ç—ã: ¬´–í–æ—Ç –ø—Ä–∏–º–µ—Ä¬ª, ¬´–û–∫–µ–π, –Ω–æ¬ª, ¬´–°–µ—Ä—å—ë–∑–Ω–æ?¬ª, ¬´–ó–Ω–∞–µ—Ç–µ —á—Ç–æ?¬ª
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã: ¬´–∑–∞ 6 –º–µ—Å—è—Ü–µ–≤¬ª, ¬´3 –∏–∑ 5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤¬ª, ¬´—Ä–æ—Å—Ç –Ω–∞ 40%¬ª
- –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ê–±–∑–∞—Ü—ã –ø–æ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è max.
- –ù–∏–∫–∞–∫–æ–π –≤–æ–¥—ã. –ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ ‚Äî —Ü–µ–Ω–Ω–æ—Å—Ç—å.
- –ë–æ–ª—å—à–µ ¬´—è –≤–∏–¥–µ–ª / —è –∑–∞–º–µ—Ç–∏–ª / —É –Ω–∞—Å –±—ã–ª–æ¬ª –∏ –º–µ–Ω—å—à–µ ¬´–≤–∞–º –Ω—É–∂–Ω–æ / –≤—ã –¥–æ–ª–∂–Ω—ã¬ª

–î–ª–∏–Ω–∞: 1500-2500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è LinkedIn.
–ü–æ–¥–ø–∏—Å—å –≤ –∫–æ–Ω—Ü–µ (–î–û CTA-–≤–æ–ø—Ä–æ—Å–∞): ¬´‚Äî {author}\n–°–ë–û–†–ö–ê ‚Äî –∫–ª—É–± –∫–∞—Ä—å–µ—Ä–Ω–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã¬ª"""

        user_prompt = f"–ù–∞–ø–∏—à–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è {platform.upper()} –Ω–∞ —Ç–µ–º—É: {topic}"

        result = _call_llm(user_prompt, system=system_prompt, max_tokens=2000)

        if not result:
            return "Error: LLM call failed. Check OPENROUTER_API_KEY or GROQ_API_KEY."

        # Strip CJK characters and other non-Cyrillic garbage
        result = _strip_non_cyrillic(result)

        # Auto-critique
        critique = _critique_content(result, topic, author)

        # If passed, return
        if critique["passed"]:
            return (
                f"POST GENERATED (score: {critique['overall_score']:.2f} ‚úÖ)\n"
                f"Author: {author}\nTopic: {topic}\nLength: {len(result)} chars\n"
                f"---\n{result}"
            )

        # Try one refine iteration
        refined = self._refine(result, topic, author)
        return refined

    def _refine(self, content: str, topic: str, author: str) -> str:
        """Refine content using LLM."""
        critique = _critique_content(content, topic, author)

        if critique["passed"]:
            return (
                f"CONTENT ALREADY GOOD (score: {critique['overall_score']:.2f} ‚úÖ)\n"
                f"---\n{content}"
            )

        issues_text = "\n".join(f"- {i}" for i in critique["issues"])

        prompt = f"""–£–ª—É—á—à–∏ —ç—Ç–æ—Ç –ø–æ—Å—Ç –¥–ª—è –°–ë–û–†–ö–ò (–∫–ª—É–± –∫–∞—Ä—å–µ—Ä–Ω–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã).

–¢–ï–ö–£–©–ò–ï –ü–†–û–ë–õ–ï–ú–´:
{issues_text}

–¢–ï–ö–£–©–ò–ô SCORE: {critique['overall_score']:.2f}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ 6 —á–∞—Å—Ç–µ–π: –∫—Ä—é—á–æ–∫, –ø—Ä–æ–±–ª–µ–º–∞, –∏—Å—Ç–æ—Ä–∏—è, –∏–Ω—Å–∞–π—Ç, —Ä–µ—à–µ–Ω–∏–µ, –≤—ã–≤–æ–¥
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–∏–º–µ—Ä—ã
- –ü—Ä—è–º–æ–π —É–≤–µ—Ä–µ–Ω–Ω—ã–π —Ç–æ–Ω
- –î–ª–∏–Ω–∞ 1500-2500 —Å–∏–º–≤–æ–ª–æ–≤
- –ü–æ–¥–ø–∏—Å—å: ¬´‚Äî {author}\n–°–ë–û–†–ö–ê ‚Äî –∫–ª—É–± –∫–∞—Ä—å–µ—Ä–Ω–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã¬ª
- –ó–ê–ü–†–ï–©–ï–ù–û: {', '.join(FORBIDDEN_PHRASES[:6])}

–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–ö–°–¢:
{content}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""

        refined = _call_llm(prompt, max_tokens=2000)

        if not refined:
            return f"REFINE FAILED (original score: {critique['overall_score']:.2f})\n---\n{content}"

        # Strip CJK characters
        refined = _strip_non_cyrillic(refined)

        new_critique = _critique_content(refined, topic, author)

        status = "‚úÖ" if new_critique["passed"] else "‚ö†Ô∏è"
        return (
            f"REFINED {status} (score: {critique['overall_score']:.2f} ‚Üí {new_critique['overall_score']:.2f})\n"
            f"Length: {len(refined)} chars\n"
            f"---\n{refined}"
        )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Tool 2: Yuki Memory
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class YukiMemoryInput(BaseModel):
    action: str = Field(
        ...,
        description=(
            "Action: 'get_rules' (content rules), "
            "'get_brand_voice' (author profiles), "
            "'get_topics' (content categories), "
            "'get_forbidden' (banned phrases), "
            "'record_generation' (save post ‚Äî needs data as JSON string), "
            "'record_feedback' (save feedback ‚Äî needs data as JSON string), "
            "'get_stats' (generation/feedback statistics)"
        ),
    )
    data: Optional[str] = Field(None, description="JSON string with data for record actions")


class YukiMemory(BaseTool):
    name: str = "Yuki Memory"
    description: str = (
        "4-layer memory system: procedural (rules), semantic (brand voice), "
        "episodic (history), working (state). "
        "Actions: get_rules, get_brand_voice, get_topics, get_forbidden, "
        "record_generation, record_feedback, get_stats."
    )
    args_schema: Type[BaseModel] = YukiMemoryInput

    def _run(self, action: str, data: str = None) -> str:
        mem = _memory_dir()

        if action == "get_rules":
            rules = _load_json(os.path.join(mem, "procedural", "rules.json"))
            items = rules.get("rules", [])
            if not items:
                return "No rules found."
            lines = ["CONTENT RULES:"]
            for r in items:
                lines.append(f"  [{r.get('category', '?')}] {r.get('rule', '?')} (confidence: {r.get('confidence', 0)})")
            return "\n".join(lines)

        if action == "get_brand_voice":
            bv = _load_json(os.path.join(mem, "semantic", "brand_voice.json"))
            authors = bv.get("authors", {})
            tone = bv.get("tone", {})
            lines = ["BRAND VOICE:"]
            for key, info in authors.items():
                lines.append(f"  {info.get('name', key)}: {info.get('voice', '?')}")
                lines.append(f"    Role: {info.get('role', '?')}")
            lines.append(f"  DO: {', '.join(tone.get('do', []))}")
            lines.append(f"  DON'T: {', '.join(tone.get('dont', []))}")
            sig = bv.get("signature", "")
            if sig:
                lines.append(f"  Signature: {sig}")
            return "\n".join(lines)

        if action == "get_topics":
            topics = _load_json(os.path.join(mem, "semantic", "topics.json"))
            cats = topics.get("categories", {})
            lines = ["CONTENT TOPICS:"]
            for cat, keywords in cats.items():
                lines.append(f"  {cat}: {', '.join(keywords[:5])}")
            trending = topics.get("trending", [])
            if trending:
                lines.append(f"  Trending: {', '.join(trending)}")
            return "\n".join(lines)

        if action == "get_forbidden":
            vocab = _load_json(os.path.join(mem, "semantic", "vocabulary.json"))
            forbidden = vocab.get("forbidden_phrases", [])
            recommended = vocab.get("recommended_phrases", [])
            lines = ["VOCABULARY:"]
            lines.append(f"  FORBIDDEN ({len(forbidden)}): {', '.join(forbidden)}")
            lines.append(f"  RECOMMENDED ({len(recommended)}): {', '.join(recommended)}")
            return "\n".join(lines)

        if action == "record_generation":
            if not data:
                return "Error: need data (JSON string)"
            try:
                record = json.loads(data)
            except json.JSONDecodeError:
                record = {"content_preview": data[:200], "raw": True}

            record["timestamp"] = datetime.now().isoformat()
            # Append to JSONL file
            date_str = datetime.now().strftime("%Y-%m-%d")
            path = os.path.join(mem, "episodic", "generations", f"{date_str}.jsonl")
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, "a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
            return f"Generation recorded ({date_str})"

        if action == "record_feedback":
            if not data:
                return "Error: need data (JSON string)"
            try:
                record = json.loads(data)
            except json.JSONDecodeError:
                record = {"feedback": data, "raw": True}

            record["timestamp"] = datetime.now().isoformat()
            date_str = datetime.now().strftime("%Y-%m-%d")
            path = os.path.join(mem, "episodic", "feedback", f"{date_str}.jsonl")
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, "a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
            return f"Feedback recorded ({date_str})"

        if action == "get_stats":
            gen_dir = os.path.join(mem, "episodic", "generations")
            fb_dir = os.path.join(mem, "episodic", "feedback")

            gen_count = 0
            fb_count = 0
            if os.path.isdir(gen_dir):
                for f in os.listdir(gen_dir):
                    if f.endswith(".jsonl"):
                        with open(os.path.join(gen_dir, f), "r") as fh:
                            gen_count += sum(1 for _ in fh)
            if os.path.isdir(fb_dir):
                for f in os.listdir(fb_dir):
                    if f.endswith(".jsonl"):
                        with open(os.path.join(fb_dir, f), "r") as fh:
                            fb_count += sum(1 for _ in fh)

            state = _load_json(os.path.join(mem, "working", "state.json"))
            autonomy = state.get("agent", {}).get("autonomy_name", "DRAFT")

            return (
                f"YUKI STATS:\n"
                f"  Generations: {gen_count}\n"
                f"  Feedback entries: {fb_count}\n"
                f"  Autonomy level: {autonomy}\n"
                f"  Memory files: rules, brand_voice, topics, vocabulary"
            )

        return f"Unknown action: {action}"


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Tool 3: LinkedIn Publisher (Modern REST API)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_LINKEDIN_API_VERSION = "202502"
_LINKEDIN_BASE = "https://api.linkedin.com"


class LinkedInPublisherInput(BaseModel):
    action: str = Field(
        ...,
        description=(
            "Action: 'publish_text' (text post ‚Äî needs text), "
            "'publish_image' (post with image ‚Äî needs text + image_url), "
            "'check_token' (check if LinkedIn is configured), "
            "'status' (LinkedIn integration status)"
        ),
    )
    text: Optional[str] = Field(None, description="Post text (max 3000 chars)")
    image_url: Optional[str] = Field(None, description="Image URL to attach (for publish_image)")


class LinkedInPublisherTool(BaseTool):
    name: str = "LinkedIn Publisher"
    description: str = (
        "Publishes posts to LinkedIn via modern REST API. "
        "Actions: publish_text, publish_image, check_token, status. "
        "Supports text posts (up to 3000 chars) and image posts."
    )
    args_schema: Type[BaseModel] = LinkedInPublisherInput

    def _run(self, action: str, text: str = None, image_url: str = None) -> str:
        from urllib.request import urlopen, Request
        from urllib.error import HTTPError

        access_token = os.getenv("LINKEDIN_ACCESS_TOKEN", "")
        person_id = os.getenv("LINKEDIN_PERSON_ID", "")

        if action == "status":
            configured = bool(access_token and person_id)
            return (
                f"LINKEDIN STATUS:\n"
                f"  Configured: {'‚úÖ Yes' if configured else '‚ùå No'}\n"
                f"  Token: {'Set' if access_token else 'MISSING'}\n"
                f"  Person ID: {'Set' if person_id else 'MISSING'}\n"
                f"  API: REST /rest/posts (v{_LINKEDIN_API_VERSION})"
            )

        if action == "check_token":
            if not access_token:
                return "‚ùå LINKEDIN_ACCESS_TOKEN not set"
            try:
                req = Request(
                    f"{_LINKEDIN_BASE}/v2/userinfo",
                    headers={
                        "Authorization": f"Bearer {access_token}",
                        "LinkedIn-Version": _LINKEDIN_API_VERSION,
                    },
                )
                with urlopen(req, timeout=10) as resp:
                    data = json.loads(resp.read().decode("utf-8"))
                    name = data.get("name", "Unknown")
                    return f"‚úÖ LinkedIn token valid. User: {name}"
            except HTTPError as e:
                if e.code == 401:
                    return "‚ùå LinkedIn token EXPIRED. Refresh at https://linkedin.com/developers/tools/oauth"
                return f"‚ùå LinkedIn error: {e.code}"
            except Exception as e:
                return f"‚ùå LinkedIn error: {e}"

        if action in ("publish_text", "publish"):
            if not text:
                return "Error: need text to publish"
            if not access_token or not person_id:
                return "‚ùå LinkedIn not configured. Set LINKEDIN_ACCESS_TOKEN and LINKEDIN_PERSON_ID."
            if len(text) > 3000:
                text = text[:2997] + "..."
            return self._publish_post(access_token, person_id, text)

        if action == "publish_image":
            if not text:
                return "Error: need text for image post"
            if not image_url:
                return "Error: need image_url for image post"
            if not access_token or not person_id:
                return "‚ùå LinkedIn not configured. Set LINKEDIN_ACCESS_TOKEN and LINKEDIN_PERSON_ID."
            if len(text) > 3000:
                text = text[:2997] + "..."
            return self._publish_image_post(access_token, person_id, text, image_url)

        return f"Unknown action: {action}. Use: publish_text, publish_image, check_token, status"

    def _publish_post(self, token: str, person_id: str, text: str) -> str:
        """Publish a text-only post via modern REST API."""
        from urllib.request import urlopen, Request
        from urllib.error import HTTPError

        payload = {
            "author": f"urn:li:person:{person_id}",
            "commentary": text,
            "visibility": "PUBLIC",
            "distribution": {
                "feedDistribution": "MAIN_FEED",
                "targetEntities": [],
                "thirdPartyDistributionChannels": [],
            },
            "lifecycleState": "PUBLISHED",
        }

        try:
            req = Request(
                f"{_LINKEDIN_BASE}/rest/posts",
                data=json.dumps(payload).encode("utf-8"),
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json",
                    "LinkedIn-Version": _LINKEDIN_API_VERSION,
                    "X-Restli-Protocol-Version": "2.0.0",
                },
                method="POST",
            )
            with urlopen(req, timeout=30) as resp:
                post_id = resp.headers.get("x-restli-id", "")
                url = f"https://www.linkedin.com/feed/update/{post_id}" if post_id else ""
                return f"‚úÖ Published to LinkedIn!\nPost ID: {post_id}\nURL: {url}"
        except HTTPError as e:
            error_body = e.read().decode("utf-8") if e.fp else ""
            if e.code == 401:
                return "‚ùå LinkedIn token EXPIRED. Refresh at https://linkedin.com/developers/tools/oauth"
            return f"‚ùå LinkedIn publish error: HTTP {e.code}\n{error_body[:200]}"
        except Exception as e:
            return f"‚ùå LinkedIn publish error: {e}"

    def _publish_image_post(self, token: str, person_id: str, text: str, image_url: str) -> str:
        """Publish a post with image via modern REST API (3-step: init upload, upload binary, create post)."""
        from urllib.request import urlopen, Request
        from urllib.error import HTTPError

        # Step 1: Initialize image upload
        init_payload = {
            "initializeUploadRequest": {
                "owner": f"urn:li:person:{person_id}",
            }
        }
        try:
            req = Request(
                f"{_LINKEDIN_BASE}/rest/images?action=initializeUpload",
                data=json.dumps(init_payload).encode("utf-8"),
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json",
                    "LinkedIn-Version": _LINKEDIN_API_VERSION,
                },
                method="POST",
            )
            with urlopen(req, timeout=15) as resp:
                init_data = json.loads(resp.read().decode("utf-8"))
                value = init_data.get("value", {})
                upload_url = value.get("uploadUrl", "")
                image_urn = value.get("image", "")
                if not upload_url or not image_urn:
                    return f"‚ùå LinkedIn image init failed: no uploadUrl or image URN"
        except HTTPError as e:
            error_body = e.read().decode("utf-8") if e.fp else ""
            return f"‚ùå LinkedIn image init error: HTTP {e.code}\n{error_body[:200]}"
        except Exception as e:
            return f"‚ùå LinkedIn image init error: {e}"

        # Step 2: Download image and upload to LinkedIn
        try:
            img_req = Request(image_url, headers={"User-Agent": "ZininCorp/1.0"})
            with urlopen(img_req, timeout=30) as img_resp:
                image_data = img_resp.read()

            upload_req = Request(
                upload_url,
                data=image_data,
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/octet-stream",
                },
                method="PUT",
            )
            with urlopen(upload_req, timeout=60) as _:
                pass
        except Exception as e:
            return f"‚ùå LinkedIn image upload error: {e}"

        # Step 3: Create post with image
        post_payload = {
            "author": f"urn:li:person:{person_id}",
            "commentary": text,
            "visibility": "PUBLIC",
            "distribution": {
                "feedDistribution": "MAIN_FEED",
                "targetEntities": [],
                "thirdPartyDistributionChannels": [],
            },
            "content": {
                "media": {
                    "title": "Image",
                    "id": image_urn,
                }
            },
            "lifecycleState": "PUBLISHED",
        }

        try:
            req = Request(
                f"{_LINKEDIN_BASE}/rest/posts",
                data=json.dumps(post_payload).encode("utf-8"),
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json",
                    "LinkedIn-Version": _LINKEDIN_API_VERSION,
                    "X-Restli-Protocol-Version": "2.0.0",
                },
                method="POST",
            )
            with urlopen(req, timeout=30) as resp:
                post_id = resp.headers.get("x-restli-id", "")
                url = f"https://www.linkedin.com/feed/update/{post_id}" if post_id else ""
                return f"‚úÖ Published to LinkedIn with image!\nPost ID: {post_id}\nURL: {url}"
        except HTTPError as e:
            error_body = e.read().decode("utf-8") if e.fp else ""
            if e.code == 401:
                return "‚ùå LinkedIn token EXPIRED."
            return f"‚ùå LinkedIn image post error: HTTP {e.code}\n{error_body[:200]}"
        except Exception as e:
            return f"‚ùå LinkedIn image post error: {e}"


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Tool 3b: Threads Publisher (Meta API)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_THREADS_BASE = "https://graph.threads.net/v1.0"


class ThreadsPublisherInput(BaseModel):
    action: str = Field(
        ...,
        description=(
            "Action: 'publish_text' (text post ‚Äî needs text), "
            "'publish_image' (image post ‚Äî needs text + image_url), "
            "'publish_carousel' (carousel ‚Äî needs text + image_urls comma-separated), "
            "'check_token' (verify Threads is configured), "
            "'status' (Threads integration status)"
        ),
    )
    text: Optional[str] = Field(None, description="Post text (max 500 chars)")
    image_url: Optional[str] = Field(None, description="Image URL for single image post")
    image_urls: Optional[str] = Field(
        None, description="Comma-separated image URLs for carousel (2-20 images)"
    )


class ThreadsPublisherTool(BaseTool):
    name: str = "Threads Publisher"
    description: str = (
        "Publishes posts to Threads (Meta) via official API. "
        "Actions: publish_text, publish_image, publish_carousel, check_token, status. "
        "Supports text, image, and carousel posts."
    )
    args_schema: Type[BaseModel] = ThreadsPublisherInput

    def _run(
        self,
        action: str,
        text: str = None,
        image_url: str = None,
        image_urls: str = None,
    ) -> str:
        from urllib.request import urlopen, Request
        from urllib.error import HTTPError
        from urllib.parse import urlencode
        import time

        access_token = os.getenv("THREADS_ACCESS_TOKEN", "")
        user_id = os.getenv("THREADS_USER_ID", "")

        if action == "status":
            configured = bool(access_token and user_id)
            return (
                f"THREADS STATUS:\n"
                f"  Configured: {'‚úÖ Yes' if configured else '‚ùå No'}\n"
                f"  Token: {'Set' if access_token else 'MISSING'}\n"
                f"  User ID: {'Set' if user_id else 'MISSING'}\n"
                f"  API: {_THREADS_BASE}"
            )

        if action == "check_token":
            if not access_token or not user_id:
                return "‚ùå THREADS_ACCESS_TOKEN or THREADS_USER_ID not set"
            try:
                params = urlencode({"access_token": access_token})
                req = Request(f"{_THREADS_BASE}/{user_id}?{params}")
                with urlopen(req, timeout=10) as resp:
                    data = json.loads(resp.read().decode("utf-8"))
                    name = data.get("name", data.get("username", "Unknown"))
                    return f"‚úÖ Threads token valid. User: {name}"
            except HTTPError as e:
                if e.code == 190 or e.code == 401:
                    return "‚ùå Threads token EXPIRED. Re-authorize at developers.meta.com"
                return f"‚ùå Threads error: HTTP {e.code}"
            except Exception as e:
                return f"‚ùå Threads error: {e}"

        if action in ("publish_text", "publish"):
            if not text:
                return "Error: need text to publish"
            if not access_token or not user_id:
                return "‚ùå Threads not configured. Set THREADS_ACCESS_TOKEN and THREADS_USER_ID."
            if len(text) > 500:
                text = text[:497] + "..."
            return self._publish_text(access_token, user_id, text)

        if action == "publish_image":
            if not text:
                return "Error: need text for image post"
            if not image_url:
                return "Error: need image_url for image post"
            if not access_token or not user_id:
                return "‚ùå Threads not configured. Set THREADS_ACCESS_TOKEN and THREADS_USER_ID."
            if len(text) > 500:
                text = text[:497] + "..."
            return self._publish_image(access_token, user_id, text, image_url)

        if action == "publish_carousel":
            if not text:
                return "Error: need text for carousel"
            if not image_urls:
                return "Error: need image_urls (comma-separated) for carousel"
            if not access_token or not user_id:
                return "‚ùå Threads not configured. Set THREADS_ACCESS_TOKEN and THREADS_USER_ID."
            urls = [u.strip() for u in image_urls.split(",") if u.strip()]
            if len(urls) < 2:
                return "Error: carousel needs at least 2 images"
            if len(urls) > 20:
                urls = urls[:20]
            if len(text) > 500:
                text = text[:497] + "..."
            return self._publish_carousel(access_token, user_id, text, urls)

        return f"Unknown action: {action}. Use: publish_text, publish_image, publish_carousel, check_token, status"

    def _create_container(self, token: str, user_id: str, params: dict) -> str:
        """Create a media container. Returns container ID or error string starting with '‚ùå'."""
        from urllib.request import urlopen, Request
        from urllib.error import HTTPError
        from urllib.parse import urlencode

        params["access_token"] = token
        url = f"{_THREADS_BASE}/{user_id}/threads"
        try:
            req = Request(
                url,
                data=urlencode(params).encode("utf-8"),
                method="POST",
            )
            with urlopen(req, timeout=30) as resp:
                data = json.loads(resp.read().decode("utf-8"))
                container_id = data.get("id", "")
                if not container_id:
                    return "‚ùå Threads: no container ID returned"
                return container_id
        except HTTPError as e:
            error_body = e.read().decode("utf-8") if e.fp else ""
            return f"‚ùå Threads container error: HTTP {e.code}\n{error_body[:200]}"
        except Exception as e:
            return f"‚ùå Threads container error: {e}"

    def _publish_container(self, token: str, user_id: str, container_id: str) -> str:
        """Publish a media container. Returns success message or error."""
        from urllib.request import urlopen, Request
        from urllib.error import HTTPError
        from urllib.parse import urlencode
        import time

        # Wait for container processing
        time.sleep(5)

        params = {"creation_id": container_id, "access_token": token}
        url = f"{_THREADS_BASE}/{user_id}/threads_publish"
        try:
            req = Request(
                url,
                data=urlencode(params).encode("utf-8"),
                method="POST",
            )
            with urlopen(req, timeout=30) as resp:
                data = json.loads(resp.read().decode("utf-8"))
                post_id = data.get("id", "")
                return post_id
        except HTTPError as e:
            error_body = e.read().decode("utf-8") if e.fp else ""
            return f"‚ùå Threads publish error: HTTP {e.code}\n{error_body[:200]}"
        except Exception as e:
            return f"‚ùå Threads publish error: {e}"

    def _publish_text(self, token: str, user_id: str, text: str) -> str:
        """Publish a text-only Threads post."""
        container_id = self._create_container(token, user_id, {
            "media_type": "TEXT",
            "text": text,
        })
        if container_id.startswith("‚ùå"):
            return container_id

        result = self._publish_container(token, user_id, container_id)
        if result.startswith("‚ùå"):
            return result
        return f"‚úÖ Published to Threads!\nPost ID: {result}\nURL: https://www.threads.net/post/{result}"

    def _publish_image(self, token: str, user_id: str, text: str, image_url: str) -> str:
        """Publish a Threads post with image."""
        container_id = self._create_container(token, user_id, {
            "media_type": "IMAGE",
            "text": text,
            "image_url": image_url,
        })
        if container_id.startswith("‚ùå"):
            return container_id

        result = self._publish_container(token, user_id, container_id)
        if result.startswith("‚ùå"):
            return result
        return f"‚úÖ Published to Threads with image!\nPost ID: {result}\nURL: https://www.threads.net/post/{result}"

    def _publish_carousel(self, token: str, user_id: str, text: str, image_urls: list) -> str:
        """Publish a Threads carousel post (3-step: items ‚Üí carousel container ‚Üí publish)."""
        import time

        # Step 1: Create item containers
        item_ids = []
        for url in image_urls:
            item_id = self._create_container(token, user_id, {
                "media_type": "IMAGE",
                "image_url": url,
                "is_carousel_item": "true",
            })
            if item_id.startswith("‚ùå"):
                return f"‚ùå Carousel item failed: {item_id}"
            item_ids.append(item_id)
            time.sleep(1)

        # Step 2: Create carousel container
        children_str = ",".join(item_ids)
        carousel_id = self._create_container(token, user_id, {
            "media_type": "CAROUSEL",
            "text": text,
            "children": children_str,
        })
        if carousel_id.startswith("‚ùå"):
            return carousel_id

        # Step 3: Publish
        result = self._publish_container(token, user_id, carousel_id)
        if result.startswith("‚ùå"):
            return result
        return (
            f"‚úÖ Published carousel to Threads! ({len(image_urls)} images)\n"
            f"Post ID: {result}\nURL: https://www.threads.net/post/{result}"
        )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Tool 4: Podcast Script Generator
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class PodcastScriptInput(BaseModel):
    topic: str = Field(..., description="Podcast episode topic (e.g., 'AI-–∞–≥–µ–Ω—Ç—ã –≤ –±–∏–∑–Ω–µ—Å–µ')")
    duration_minutes: int = Field(
        10,
        description="Target episode duration in minutes (default 10). ~900 chars/min of speech.",
    )


class PodcastScriptGenerator(BaseTool):
    name: str = "Podcast Script Generator"
    description: str = (
        "Generates a podcast script (monologue) for AI Corporation Podcast. "
        "One voice, Russian language, conversational tone. "
        "Returns plain text ready for TTS."
    )
    args_schema: Type[BaseModel] = PodcastScriptInput

    def _run(self, topic: str, duration_minutes: int = 10) -> str:
        target_chars = duration_minutes * 900  # ~900 chars per minute of speech

        system_prompt = f"""–¢—ã ‚Äî —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç –ø–æ–¥–∫–∞—Å—Ç–∞ ¬´AI Corporation Podcast¬ª.
–ü–∏—à–µ—à—å —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è –û–î–ù–û–ì–û –≤–µ–¥—É—â–µ–≥–æ (–º–æ–Ω–æ–ª–æ–≥). –Ø–∑—ã–∫ ‚Äî —Ä—É—Å—Å–∫–∏–π.

‚ö†Ô∏è –§–û–†–ú–ê–¢: —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–∑–≤—É—á–∫–∏. –ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –±–µ–∑ markdown, –±–µ–∑ —Ç–∞–π–º–∫–æ–¥–æ–≤, –±–µ–∑ —Ä–µ–º–∞—Ä–æ–∫ —Ç–∏–ø–∞ [–ø–∞—É–∑–∞] –∏–ª–∏ (—Å–º–µ—ë—Ç—Å—è).
–¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω —á–∏—Ç–∞—Ç—å—Å—è –≤—Å–ª—É—Ö –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –∫–∞–∫ –∂–∏–≤–∞—è —Ä–µ—á—å.

üéØ –¶–ï–õ–ï–í–ê–Ø –î–õ–ò–ù–ê: ~{target_chars} —Å–∏–º–≤–æ–ª–æ–≤ ({duration_minutes} –º–∏–Ω –ø—Ä–∏ ~900 —Å–∏–º–≤–æ–ª–æ–≤/–º–∏–Ω).

üìê –°–¢–†–£–ö–¢–£–†–ê –í–´–ü–£–°–ö–ê:
1. –ö–†–Æ–ß–û–ö (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) ‚Äî —è—Ä–∫–∏–π —Ñ–∞–∫—Ç, –ø—Ä–æ–≤–æ–∫–∞—Ü–∏—è –∏–ª–∏ –≤–æ–ø—Ä–æ—Å. –°—Ä–∞–∑—É —Ü–µ–ø–ª—è–µ—Ç.
2. –í–°–¢–£–ü–õ–ï–ù–ò–ï (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) ‚Äî ¬´–ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ AI Corporation Podcast, –∏ —Å–µ–≥–æ–¥–Ω—è...¬ª
3. –û–°–ù–û–í–ù–ê–Ø –ß–ê–°–¢–¨ (3-4 —Å–µ–≥–º–µ–Ω—Ç–∞ –ø–æ 2-4 –∞–±–∑–∞—Ü–∞ –∫–∞–∂–¥—ã–π):
   - –ö–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç = –æ–¥–Ω–∞ –ø–æ–¥—Ç–µ–º–∞/–≥—Ä–∞–Ω—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–º—ã
   - –ù–∞—á–∏–Ω–∞–π —Å–µ–≥–º–µ–Ω—Ç —Å –ø–µ—Ä–µ—Ö–æ–¥–∞: ¬´–ê —Ç–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ–≥–æ–≤–æ—Ä–∏–º –æ...¬ª, ¬´–û–∫–µ–π, –Ω–æ –≤–æ—Ç —á—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ...¬ª
   - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, —Ü–∏—Ñ—Ä—ã, –∫–µ–π—Å—ã –≤ –∫–∞–∂–¥–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ
4. –í–´–í–û–î (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) ‚Äî –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Å–∞–π—Ç –≤—ã–ø—É—Å–∫–∞
5. CTA + OUTRO (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) ‚Äî ¬´–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å...¬ª, ¬´–ü–∏—à–∏—Ç–µ –≤–∞—à–∏ –º—ã—Å–ª–∏...¬ª

‚õî –ó–ê–ü–†–ï–¢–´:
- –ù–∏–∫–∞–∫–∏—Ö –º–µ–Ω—Ç–æ—Ä—Å–∫–∏—Ö –ø–æ—É—á–µ–Ω–∏–π: ¬´–≤–∞–º –Ω—É–∂–Ω–æ¬ª, ¬´–≤—ã –¥–æ–ª–∂–Ω—ã¬ª, ¬´–ø–æ–ø—Ä–æ–±—É–π—Ç–µ¬ª
- –ù–∏–∫–∞–∫–∏—Ö –∫–ª–∏—à–µ: ¬´–≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ¬ª, ¬´—Å–µ–≥–æ–¥–Ω—è –º—ã –ø–æ–≥–æ–≤–æ—Ä–∏–º¬ª
- –ù–∏–∫–∞–∫–∏—Ö –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤/CJK —Å–∏–º–≤–æ–ª–æ–≤
- –ù–∏–∫–∞–∫–∏—Ö markdown, —Å–ø–∏—Å–∫–æ–≤ —Å –±—É–ª–ª–µ—Ç–∞–º–∏, –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
- –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–≤–∞: ¬´–≤–æ–∑–º–æ–∂–Ω–æ¬ª, ¬´–Ω–∞–≤–µ—Ä–Ω–æ–µ¬ª, ¬´–∫–∞–∂–µ—Ç—Å—è¬ª, ¬´—Å–µ–∫—Ä–µ—Ç —É—Å–ø–µ—Ö–∞¬ª

‚úÖ –°–¢–ò–õ–¨:
- –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π, –∂–∏–≤–æ–π, –∫–∞–∫ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—à—å –¥—Ä—É–≥—É –∑–∞ –∫–æ—Ñ–µ
- –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (5-15 —Å–ª–æ–≤)
- –ü–∞—É–∑—ã —á–µ—Ä–µ–∑ —Ç–æ—á–∫–∏, –Ω–µ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—ã–µ
- –õ–∏—á–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è: ¬´–Ø –∑–∞–º–µ—Ç–∏–ª¬ª, ¬´–£ –Ω–∞—Å –±—ã–ª —Å–ª—É—á–∞–π¬ª, ¬´–û–¥–∏–Ω –º–æ–π –∑–Ω–∞–∫–æ–º—ã–π¬ª
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã
- –£–º–µ—Å—Ç–Ω—ã–π —é–º–æ—Ä –∏ –∏—Ä–æ–Ω–∏—è"""

        user_prompt = f"–ù–∞–ø–∏—à–∏ —Å—Ü–µ–Ω–∞—Ä–∏–π –ø–æ–¥–∫–∞—Å—Ç–∞ –Ω–∞ —Ç–µ–º—É: {topic}"

        result = _call_llm(user_prompt, system=system_prompt, max_tokens=4000)

        if not result:
            return "Error: LLM call failed. Check OPENROUTER_API_KEY or GROQ_API_KEY."

        # Clean up
        result = _strip_non_cyrillic(result)

        char_count = len(result)
        est_minutes = char_count / 900

        return (
            f"PODCAST SCRIPT GENERATED\n"
            f"Topic: {topic}\n"
            f"Length: {char_count} chars (~{est_minutes:.1f} min)\n"
            f"Target: {target_chars} chars (~{duration_minutes} min)\n"
            f"---\n{result}"
        )
